import os 
import os 
import math 
from inspect import isfunction 
from functools import partial

%matplotlib inline 
import matplotlib.pyplot as plt 
from tqdm.auto import tqdm
from einops import rearrange, reduce 
from einops.layers.torch import Rearrange

import torch 
from torch import nn, einsum 
import torch.nn.functional as F 



def exists(x):
    return x is not None 

def default(val,d):
    if exists(val):
        return val 
    return d() if isfunction(d) else d 

def num_to_groups(num,divisor):
    groups = num//divisor 
    remainder = num%divisor 
    arr = [divisor]*groups 
    if remainder != 0:
        arr.append(remainder)
    return arr

class Residual(nn.Module):
    def __init__(self,fn):
        super().__init__()
        self.fn = fn
    
    def forward(self,x,**kwargs):
        return self.fn(x,**kwargs) + x

def Upsample(dim, dim_out=None, mode='nearest', align_corners=None):
    return nn.Sequential(nn.Upsample(scale_factor=2, mode=mode, align_corners=align_corners),
                         nn.Conv2d(dim, default(dim_out, dim), 3, padding=1))
        
def Downsample(dim, dim_out=None, mode='nearest', align_corners=None):
    #no strided conv or padding 
    return nn.Sequential( Rearrange("b c (h p1) (w p2) -> b (c p1 p2) h w", p1=2, p2=2),
                         nn.Conv2d(dim*4, default(dim_out, dim), 1))


# Positional Embedding 
"""
As the parameters of the neural network are shared across time-step (noise level), 
the authors employ sinusoidal position embeddings to encode t. 
This makes the neural network "know" at which particular time step (noise level) it is operating, for every image in a batch.

The SinusoidalPositionEmbeddings module takes a tensor of shape (batch_size, 1) as input (i.e. the noise levels of several noisy images in a batch), 
and turns this into a tensor of shape (batch_size, dim), with dim being the dimensionality of the position embeddings. 
This is then added to each residual block, as we will see further.
"""

class SinusoidalPositionEmbeddings(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self,time):
        device = time.device
        half_dim = self.dim // 2
        
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings        
        

# Wide ResNet Block with weight standardization

class WeightStandardizedConv2d(nn.Conv2d):
    """
    https://arxiv.org/abs/1903.10520
    weight standardization purportedly works synergistically with group normalization
    """

    def forward(self, x):
        eps = 1e-5 if x.dtype == torch.float16 else 1e-8

        weight = self.weight
        mean = reduce(weight, "o ... -> o 1 1 1", "mean")
        var = reduce(weight, "o ... -> o 1 1 1", partial(torch.var, unbiased=False))
        normalized_weight = (weight - mean) * (var + eps).rsqrt()

        return F.conv2d(x, normalized_weight, self.bias, self.stride, self.padding, self.dilation, self.groups)


class Block(nn.Module):
    def __init__(self, dim, dim_out, groups=8)
        super().__init__()
        self.norm = nn.GroupNorm(groups, dim_out)
        self.act = nn.SiLU()
        self.conv = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)
        
    def forward(self, x, scale_shift=None):
        x = self.conv(x)
        x = self.norm(x)
        
        if exists(scale_shift):
            scale, shift = scale_shift
            x = x * (scale+1) + shift

        x = self.act(x)        
        
        return x


class ResnetBlock(nn.Module):

    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):
        super().__init__()
        self.mlp =  (nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))
                     if exists(time_emb_dim) else None)
        
        self.block1 = Block(dim, dim_out, groups=groups)
        self.block2 = Block(dim_out, dim_out, groups=groups)
        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()

    def forward(self, x, time_emb=None):
        scale_shift = None 

        if exists(self.mlp) and exists(time_emb):
            time_emb = self.mlp(time_emb)
            time_emb = rearrange(time_emb, "b c -> b c 1 1")
            scale_shift = time_emb.chunk(2, dim=1)

        h = self.block1(x, scale_shift=scale_shift)
        h = self.block2(h)
        return self.res_conv(x) + h


# Attention Modules 

class Attention(nn.Module):
    def __init__(self, dim, heads=4, dim_head=32):
        super().__init__()
        self.heads = heads
        self.scale = dim_head ** -0.5
        inner_dim = dim_head * heads

        self.to_qkv = nn.Conv2d(dim, inner_dim * 3, bias=False)
        self.to_out = nn.Conv2d(inner_dim, dim,1)

    def forward(self, x):
        b, c, h, w = x.shape
        qkv = self.to_qkv(x).chunk(3, dim=1)
        q, k, v = map(lambda t: rearrange(t, "b (h c) x y -> b h c (x y)", h=self.heads), qkv)

        q = q * self.scale

        sim = einsum("b h d i, b h d j -> b h i j", q, k)
        sim = sim - sim.amax(dim=-1, keepdim=True).detach()
        attn = sim.softmax(dim=-1)

        out = einsum("b h i j, b h d j -> b h i d", attn, v)
        out = rearrange(out, "b h (x y) d-> b (h d) x y", x=h, y=w)
        return self.to_out(out)
    
class LinearAttention(nn.module):
    def __init__(self, dim, heads=4, dim_head=32):
        super().__init__()
        self.heads = heads
        self.scale = dim_head ** -0.5
        inner_dim = dim_head * heads

        self.to_qkv = nn.Conv2d(dim, inner_dim * 3, bias=False)
        self.to_out = nn.Sequential(nn.Conv2d(inner_dim, dim,1), nn.GroupNorm(1, dim))

    def forward(self, x):
        b, c, h, w = x.shape
        qkv = self.to_qkv(x).chunk(3, dim=1)

        q, k, v = map(lambda t: rearrange(t, "b (h c) x y -> b h c (x y)", h=self.heads), qkv)
        
        q = q.softmax(dim=-2)
        k = k.softmax(dim=-1)

        q = q * self.scale
        context = torch.einsum("b h d n, b h e n -> b h d e", k, v)

        out = torch.einsum("b h d e, b h d n -> b h e n", context, q)
        out = rearrange(out, "b h c (x y)-> b (h c) x y", h =self.heads, x=h, y=w)
        return self.to_out(out)

  
# Group Normalization

class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.GroupNorm(1, dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        x = self.norm(x)
        return self.fn(x, **kwargs)
    

    

